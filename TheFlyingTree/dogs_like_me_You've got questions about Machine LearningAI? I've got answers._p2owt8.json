{"title": "You've got questions about Machine Learning/AI? I've got answers.", "name": "t3_p2owt8", "url": "https://www.reddit.com/r/TheFlyingTree/comments/p2owt8/youve_got_questions_about_machine_learningai_ive/", "selftext": "What do you wanna know? Your friendly neighborhood data scientist is here to help and nerd out with you.\n\nEDIT: Goddamn, why are all my responses so long?", "score": 12, "upvote_ratio": 1.0, "permalink": "/r/TheFlyingTree/comments/p2owt8/youve_got_questions_about_machine_learningai_ive/", "id": "p2owt8", "author": "dogs_like_me", "link_flair_text": "Discussion", "num_comments": 59, "over_18": false, "spoiler": false, "pinned": false, "locked": false, "distinguished": null, "created_utc": 1628724618.0, "comments": [{"author": "plasma_phys", "id": "h8lpyij", "score": 3, "subreddit": "TheFlyingTree", "author_flair": "Seeker #111", "submission": "p2owt8", "stickied": false, "body": "It seems like most ML/AI code is shared as interactive notebooks, e.g., Jupyter, colab, etc., as opposed to, say, sharing the codebase via github. Any idea why? In computational physics I almost universally see the latter instead of the former (although just as common is a zip file that you have to email an old person to get a copy of, haha)", "is_submitter": false, "distinguished": null, "created_utc": 1628728266.0, "parent_id": "t3_p2owt8", "replies": [{"author": "dogs_like_me", "id": "h8lx1yy", "score": 2, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "Honestly, I'm not a fan of the whole notebook thing. Not saying I don't use notebooks myself, but I basically use jupyter as an IDE and move all the code I care about into version controlled packages with tests and shit. But I'm a crotchety old guy apparently, and a lot of the community feels differently from me. A great pro-notebook activist is Jeremy Howard, the guy behind FastAI (a popular free deep learning course). On top of publishing lots of great educational materials, he's actually built some interesting tools that allow you to build well structured projects out of notebooks. If you've got an hour, here's his [I like notebooks](https://www.youtube.com/watch?v=9Q6sLbz37gk) talk. That talk was a response to another talk linked in the description that better represents my views.\n\nAnyway, I think a big part of the \"why\" is pedagogical. Notebooks are a great environment for teaching and learning, and are popular across the python ecosystem. They lend themselves well to data exploration and visualization, so they've also become an important tool for analysts. A lot of people these days get into data science through accelerated programs like boot camps or that otherwise de-emphasize the importance of software engineering best practices. As a consequence, a lot of data scientists are just bad software engineers, and they don't grow past doing things the way they first learned them (i.e. in notebooks).\n\nThe other side of the coin is compute. A lot of the most interesting ML toys require fancy GPUs to run at a reasonable speed. I do this stuff professionally and I don't own a system with appropriate hardware to tinker on my own. If I want to play around with new research, the only way for me to do it for free is to crack open a notebook on google colab or kaggle because they are kind of enough to offer a small amount of free GPU compute. This issue isn't as bad as it used to be and we can get away with a lot more on just a CPU now, but honestly the GPU requirement used to be so hard-line that a lot of ML practitioners outright avoided deep learning up until a few years ago because it was perceived as prohibitively expensive. I remember when the FastAI course was first released, colab didn't exist so they had scripts for spinning up resources on AWS. I remember tinkering with the scripts a little and then burning a hole in my wallet for a few months because I didn't tear everything down properly when I was done playing. Colab is an amazing resource, and the access it provides to free GPU compute definitely incentivizes researchers to at least publish demo notebooks.", "is_submitter": true, "distinguished": null, "created_utc": 1628731612.0, "parent_id": "t1_h8lpyij", "replies": [{"author": "plasma_phys", "id": "h8p9kzc", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Seeker #111", "submission": "p2owt8", "stickied": false, "body": "Thanks! I'll give that talk a listen, I'm interested to hear the perspective of a notebook advocate. Computer time is a key thing that didn't occur to me. In the world of physics, usually the only way to run expensive codes is to have a contract granting you, say, a million CPU hours at some DOE computer - although with GPU-capabilities veeeery slowly inching their way into the field, it might make more sense to take advantage of those free resources like Google colab... definitely food for thought.", "is_submitter": false, "distinguished": null, "created_utc": 1628801566.0, "parent_id": "t1_h8lx1yy", "replies": [{"author": "dogs_like_me", "id": "h9figpe", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "I think there are two reasons HPC in physics generally has stayed on CPU: particle-based simulations, and diff eq solvers. Massive simulations I think are staying on cpu compute for now, but deep learning techniques have recently made some interesting progress with neural ode/pde solvers. Worth checking out if thats how you parameterize your work.", "is_submitter": true, "distinguished": null, "created_utc": 1629307407.0, "parent_id": "t1_h8p9kzc", "replies": []}]}]}]}, {"author": "FalconRelevant", "id": "h8mtmkw", "score": 3, "subreddit": "TheFlyingTree", "author_flair": "Archon", "submission": "p2owt8", "stickied": false, "body": "So I just finished the basics of Neural Networks, what next? Can you recommend books? I prefer Python, though willing to explore more specialized options.", "is_submitter": false, "distinguished": null, "created_utc": 1628758755.0, "parent_id": "t3_p2owt8", "replies": [{"author": "dogs_like_me", "id": "h8new2b", "score": 3, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "* [Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/) is a great book for general/traditional ML (everything isn't neural networks). I used to refer to this book as my bible.\n* Reinforcement Learning is an incredibly powerful paradigm that has applications in a lot of surprising places. If you want to dig deeper into the topic, [Sutton & Barto](http://incompleteideas.net/book/the-book-2nd.html) is one of the best textbooks available and is also free to download.\n* If you're interested in NLP... the field is changing so fast I'm not sure a book would be helpful. I used to recommend [NLTK](https://www.nltk.org/book/) but it focuses on a lot of older methods. Still might be a good entry point and introduction. The [Spacy](https://course.spacy.io/en/) course strikes a decent balance between old school and new school. For strictly modern tooling, the [huggingface course](https://huggingface.co/course/chapter1) is where it's at, but currently is limited to introductory material and the rest of the huggingface docs might be hard to navigate. \n* [Murhpy's *Machine Learning: a Probabilistic Perspective*](https://probml.github.io/pml-book/) is another spectacular general ML book. It was published just before the deep learning revolution really gained steam so those methods aren't well represented in the book (same goes for ESL), but he's working on an updated edition that will be released in two volumes. The first volume and the original book are both available to download from that link.\n* If you wanna get Bayesian, [Gelman's *Bayesian Data Analysis*](http://www.stat.columbia.edu/~gelman/book/) is a popular entry point and again, free! Thank god for all these amazing free, super high quality resources, right? I'm also a HUGE fan of Gelman's book on [Hierarchical Modeling](http://www.stat.columbia.edu/~gelman/arm/), but that one unfortunately isn't free. Totally worth it though, even if you just wanna better understand regression with GLMs.\n* [Geometric Deep Learning](https://geometricdeeplearning.com/) is a fledgling area of research that's really interesting, but a bit more mathy/theoretical. I actually just started that course myself (free book AND lectures via that link). Seems to be the new hotness for graph analytics.\n* Another course I recently started following is [Berkeley's *Deep Unsupervised Learning*](https://www.youtube.com/playlist?list=PLwRJQ4m4UJjPiJP3691u-qWwPGVKzSlNP). This one's just youtube lectures, but the course website also has [assignments](https://sites.google.com/view/berkeley-cs294-158-sp20/home) you can complete. The course covers a lot of really interesting areas of research that have recently seen significant developments.\n* Here's a free course + book on [Causal Inference](https://www.bradyneal.com/causal-inference-course), which is a whole rabbit hole unto itself. I'm not super familiar with the content or the author, but it was authored by a researcher who's being advised by Yoshua Bengio (a big name in deep learning/ML) so it's probably good stuff. If you don't mind paying for it, [Judea Pearl's *The Book of Why*](https://www.amazon.com/Book-Why-Science-Cause-Effect/dp/1541698967/) is a really accessible and enjoyable introduction to the topic. For his more advanced material, [Pearl's *Causality*](https://www.amazon.com/gp/product/B00AKE1VYK) is one of the seminal books in the field. \n* Wish I had a recommendation for time series modeling. Maybe poke around the docs for [Greykite](https://engineering.linkedin.com/blog/2021/greykite--a-flexible--intuitive--and-fast-forecasting-library) or [Prophet](https://facebook.github.io/prophet/) as an entry point.", "is_submitter": true, "distinguished": null, "created_utc": 1628773524.0, "parent_id": "t1_h8mtmkw", "replies": []}]}, {"author": "Zealousideal_Mix_629", "id": "h8lji3j", "score": 2, "subreddit": "TheFlyingTree", "author_flair": "Acolyte #13 | Legio Virtus", "submission": "p2owt8", "stickied": false, "body": "How do we stop them before it's too late.", "is_submitter": false, "distinguished": null, "created_utc": 1628725246.0, "parent_id": "t3_p2owt8", "replies": [{"author": "dogs_like_me", "id": "h8lnv85", "score": 5, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "Anytime a website asks you for demographic information solely for the purpose of personalizing an experience for you, lie. Fuck with their training data. It's the only way. \n\nPS: Reddit has also become an important training dataset. You probably have more influence on modern AI models than most people just from your participation here. The \"fuck with their training data\" advice definitely still applies here on reddit.", "is_submitter": true, "distinguished": null, "created_utc": 1628727294.0, "parent_id": "t1_h8lji3j", "replies": [{"author": "Zealousideal_Mix_629", "id": "h8lprch", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Acolyte #13 | Legio Virtus", "submission": "p2owt8", "stickied": false, "body": "I already do that, but it will not be enough, how do we go on the offensive?", "is_submitter": false, "distinguished": null, "created_utc": 1628728173.0, "parent_id": "t1_h8lnv85", "replies": [{"author": "dogs_like_me", "id": "h8lx992", "score": 3, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "Invest in rare earth magnets", "is_submitter": true, "distinguished": null, "created_utc": 1628731709.0, "parent_id": "t1_h8lprch", "replies": [{"author": "Zealousideal_Mix_629", "id": "h8lxnoc", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Acolyte #13 | Legio Virtus", "submission": "p2owt8", "stickied": false, "body": "But like you said Reddit is a training bet, so we can't use that one now, because now they know.", "is_submitter": false, "distinguished": null, "created_utc": 1628731897.0, "parent_id": "t1_h8lx992", "replies": [{"author": "dogs_like_me", "id": "h8ly1rz", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "We're safe in the private subs. They can't hear us in here.\n\n...I think.", "is_submitter": true, "distinguished": null, "created_utc": 1628732081.0, "parent_id": "t1_h8lxnoc", "replies": [{"author": "Zealousideal_Mix_629", "id": "h8ngp5z", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Acolyte #13 | Legio Virtus", "submission": "p2owt8", "stickied": false, "body": "Is it? After my last post yesterday,  I had issues using reddit. *CDN error* Coincidence?, I think not.", "is_submitter": false, "distinguished": null, "created_utc": 1628774431.0, "parent_id": "t1_h8ly1rz", "replies": [{"author": "dogs_like_me", "id": "h8njnbb", "score": 2, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "oh shit me too, you think we angered it?", "is_submitter": true, "distinguished": null, "created_utc": 1628775833.0, "parent_id": "t1_h8ngp5z", "replies": [{"author": "Zealousideal_Mix_629", "id": "h8nklyi", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Acolyte #13 | Legio Virtus", "submission": "p2owt8", "stickied": false, "body": "Can't say for sure, but looks like it.", "is_submitter": false, "distinguished": null, "created_utc": 1628776273.0, "parent_id": "t1_h8njnbb", "replies": []}]}]}]}]}]}]}, {"author": "FalconRelevant", "id": "h8mth01", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Archon", "submission": "p2owt8", "stickied": false, "body": "And then people complain why YouTube algorithms don't work.", "is_submitter": false, "distinguished": null, "created_utc": 1628758618.0, "parent_id": "t1_h8lnv85", "replies": []}, {"author": "Compassionate_Cat", "id": "h8q3izf", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "", "submission": "p2owt8", "stickied": false, "body": "> Anytime a website asks you for demographic information solely for the purpose of personalizing an experience for you, lie. Fuck with their training data. It's the only way. \n\nDo you not think programmers have accounted for this? It seems naiive to me to think one can \"out smart\" something like this, and they're actually \"paving the road to hell with good intentions\"(Or bad intentions, depending on how you're looking at this), because what likely ends up happening is AI's are not only fed information, they're trained through selection pressure to detect lies, too, making them even more unstoppable.", "is_submitter": false, "distinguished": null, "created_utc": 1628814846.0, "parent_id": "t1_h8lnv85", "replies": []}]}, {"author": "Kelyaan", "id": "h8ln2ot", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Seeker #271 | Consortii Concordus", "submission": "p2owt8", "stickied": false, "body": "Nah you don't stop them - You be kind to them so they don't kill you.", "is_submitter": false, "distinguished": null, "created_utc": 1628726914.0, "parent_id": "t1_h8lji3j", "replies": []}]}, {"author": "DLJD", "id": "h8lmn44", "score": 2, "subreddit": "TheFlyingTree", "author_flair": "", "submission": "p2owt8", "stickied": false, "body": "We hear about all the big things (like self-driving cars), but what are some of the interesting little things that ML/AI is used for that we might not even know?\n\nI read about how Apple apply it to the everyday things on their products, like better palm rejection to avoid unintended clicks/touches and gesture recognition. It caught my interest precisely because it was used for such mundane things, yet I benefit from them every day.", "is_submitter": false, "distinguished": null, "created_utc": 1628726711.0, "parent_id": "t3_p2owt8", "replies": [{"author": "dogs_like_me", "id": "h8lp5nj", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "There are lots of minor personalizations you experience that would be hard to notice even if you were looking at them. For example, when you're browsing netflix, you are probably aware that an ML model is responsible for the selection of videos you're being recommended. But beyond that, for each video, there are multiple versions of thumbnails and descriptions you could be presented with and there are models responsible for picking which description and thumbnail you see. \n\nML delivers the most value when it can be deployed at massive scale. Consider the netflix example I just gave you: the difference in your engagement when a model selects the thumbnail and description for you is probably microscopic relative to how you'd engage if netflix just relied on actual people to assign a single thumbnail and description to everything. \n\nFor any problem where ML might be applicable, you can get the majority of the value quickly from throwing the problem at subject matter experts. But when you've got the problem 90% solved and you want to push the bar up to 93%, that sort of micro-optimization is where ML shines.", "is_submitter": true, "distinguished": null, "created_utc": 1628727893.0, "parent_id": "t1_h8lmn44", "replies": []}, {"author": "dogs_like_me", "id": "h8o9sxp", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "Oh, here's a fun one you probably haven't thought about. Universities use ML to help determine who they should send acceptance letters to. You're probably picturing this as something like a \"is this student smart enough?\" classifier, which is definitely part of it, but there's a weird calibration problem in here too. What if this is a backup school for the student and they go somewhere else? How many students that they accept will go somewhere else? Do we think the student is actually likely to go here if we make then an offer? Do we need to sweeten the deal? How do we balance our scholarship offers with the expected cohort size? Stuff like that.", "is_submitter": true, "distinguished": null, "created_utc": 1628786841.0, "parent_id": "t1_h8lmn44", "replies": []}]}, {"author": "germz80", "id": "h8lozi8", "score": 2, "subreddit": "TheFlyingTree", "author_flair": "Seeker #119", "submission": "p2owt8", "stickied": false, "body": "What format is learned data stored in? Is it like a NoSQL database? A data structure that is unique to AI?", "is_submitter": false, "distinguished": null, "created_utc": 1628727816.0, "parent_id": "t3_p2owt8", "replies": [{"author": "dogs_like_me", "id": "h8lt6db", "score": 3, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "It really depends on the application. People who specialize in ML work are notoriously shitty programmers in general (maybe less so now that deep learning is being taught as like a year two undergrad CS course... FML). One of the dirty secrets of data science is that the important DS skill is SQL. Like, 90% of the work on any DS project is getting the data and making it useful. So in that sense, the data used for ML is stored in conventional data stores and formats like relational databases, image files, CSVs, etc. \n\n> What format is **learned data** stored in?\n\nI really like the way you phrased this question. Depending on the application, a lot of \"learned\" data is still stored in conventional data structures. In particular, let's say that a lot of \"extracted\" attributes can be stored in conventional structures. So for example, if I search on google for \"william clinton,\" I get a little infobox on the side containing a lot of facts about former president Bill Clinton, including when he was born, how long his term was, who his vice president and wife were, etc. What we're looking at is data that is stored in what's called an \"ontology,\" aka a \"knowledge graph.\" In this graph, there is a uniquely identified entity called \"Bill clinton.\" This entity has predicates and relationships attached to it that links it to other entities in the graph. These might look something like `(Bill Clinton:PERSON) --[MARRIED_TO]--> (Hilary Clinton:PERSON)`. \n\nStoring relational attributes like this makes it a lot easier to build that info box. But we glossed over something really magical: I didn't actually search for \"Bill Clinton.\" How did google know to show me any of that stuff? There's a really interesting history of statistical tricks and techniques from computational linguistics behind modern search technology, a lot of which still gets used today. But those \"traditional\" approaches are all currently getting their lunch eaten by deep learning. The big differentiator is that those old approaches used to require a lot more deliberate pre-processing requiring lots of steps and handling edge cases. Deep learning has redefined the paradigm with \"end-to-end\" solutions that work better than the old approaches did. The underlying magic could be considered a data structure unique to AI. Back to that google example: I typed in a search query, some ML system spat out a result corresponding to a unique entity in their knowledge graph. In between my query and the returned entity result, an ML model turned my query into a bunch of numbers and did a ton of linear algebra. The sequence of operations that was applied to that bunch of numbers was probably shaped like an hour glass: my query started with a representation that required a lot of numbers, then was compressed down into a smaller representation, and then that smaller representation was passed through operators that expanded it again producing an output. This general architecture is called an encoder-decoder. The compression is a kind of \"information bottleneck,\" and the intermediate representation produced is often called an \"embedding\" because it has a geometric interpretation: the model essentially learns how to convert my query into coordinates on the surface of some bizarre, high-dimensional shape (a manifold) such that my query is close to the corresponding coordinates for the entity \"bill clinton\" on that same weird shape. \n\nEmbeddings are crazy. Look up \"word2vec\" for entry-level material. That shit was like black magic when it was first introduced just a few years ago and it's like already archaic technology. The pace of NLP research is getting crazy.", "is_submitter": true, "distinguished": null, "created_utc": 1628729782.0, "parent_id": "t1_h8lozi8", "replies": [{"author": "germz80", "id": "h8m1bxo", "score": 2, "subreddit": "TheFlyingTree", "author_flair": "Seeker #119", "submission": "p2owt8", "stickied": false, "body": "Interesting. It sounds a bit like taking the hash of a word (but making it more linear than pseudorandom), and rather than using that to index something in a hash table, you identify entries that are close to it, and maybe see which has a higher weight or something? And whichever node has a close and weighted value gets selected, pulling the data structure? I'd guess it was a JSon, object, but maybe it's similar, not quite JSon.\n\nThanks for the explanation!", "is_submitter": false, "distinguished": null, "created_utc": 1628733864.0, "parent_id": "t1_h8lt6db", "replies": [{"author": "dogs_like_me", "id": "h8npweu", "score": 2, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "You're simultaneously really close and talking about something completely different :p\n\nSo hash tables actually work on basically the opposite principle of what I'm describing. you get an O(n) lookup because the hashing function basically transforms the word into a sample from a uniform random variable. By design, the expected distance between hashes of two \"similar\" words (by any definition we choose) is the same as the expected distance between two dissimilar words. \n\nLet's make this really concrete for a moment and talk about some text representations. An early structure for encoding semantic content of a document was called the \"bag of words\" vector model. We first pick a vocabulary to constrain ourselves to, let's say our model focuses on the most common 5000 English words. Now lets consider a document consisting of the following sentences:\n\n> Who sells seashells by the seashore? sally sells seashells by the seashore!\n\nWe construct our bag of words by counting how many times any word in our vocabulary occurs in the document. So for that document, our \"bag\" looks like: (who:1, sells:2, seashells:2, ...) you get the idea. Now, we take those counts and we use them to populate a vector where each index corresponds to a word in our vocabulary. Our vocabulary is 5000 words, so our vector is going to be a 1x5000 vector. This vector is going to be mostly zeroes, but there's also a \"1\" at the position corresponding to the word \"who\", a \"2\" at the position corresponding to the word \"sells\", etc. Let's call this vector v1.\n\nBy constructing this vector representation of the document, we have projected this document into a 5000-D space. The document was assigned a coordinate in this space given by v1. We can take any document and similarly project it into this same space now. What's different about this space and the hash table is that this there's a distance operator in this space that aligns with some of our notions of document similarity. Concretely, I can take our original document and concatenate it to itself an arbitrary number of times to create a new document. Let's say we repeated the original document k times and the bag of words vector for this new document is v2, and we necessarily have `v1*k=v2`. Because these are vectors, we can interpret this geometrically: v2 is just v1 scaled by k, which means v1 and v2 point in the same direction. If we construct a new vector from some random document with different words, it'll point in a different direction. Intuitively, this means we can use this model to roughly measure document similarity by calculating the angle between vectors in this 5000-D space! Documents that are similar will point roughly in the same direction, documents that are completely unrelated to each other will be roughly orthogonal. \n\nI mentioned word2vec earlier. In word2vec, we assign every word in our vocabulary a vector that is constructed such that words that have similar semantic meaning are \"close\" to each other in a similar vector space to what I described. This space has the added properties that you can do some weird semantic arithmetic. Like, literally: `vec(King) - vec(man) + vec(woman) = vec(Queen)` kind of stuff. \n\nConcretely, the objects I'm talking about here are arrays of floats. The bag of words example was a sparse array (most entries are zero), word2vec is a dense array. Deep learning representations are generally dense arrays. That said, [locality-sensitive hashing](https://en.wikipedia.org/wiki/Locality-sensitive_hashing) is definitely more like an embedding, and you can actually use non-locality sensitive hashes for ML via the [\"hashing trick\"](https://en.wikipedia.org/wiki/Feature_hashing). The idea behind the hashing trick is that you actually want hash collisions, such that common items will dominate the information associated with their hash, and uncommon items will share real estate with them and get to add a little bit of their information too. If their information is orthogonal to each other, they may actually be able to occupy the same hash and not really interfere with each other at all.\n\n---\n\nI guess one data structure that is sort of \"unique\" to AI is tensors. Mathematically, a tensor is a generalization of a matrix. a vector is a 1-tensor, a matrix is a 2-tensor. if you stacked matrices into a cube, you'd get a 3-tensor. If you collected 3-tensors that all had the same dimension, you'd have a 4-tensor. \n\nIn the context of ML engineering, tensors are numerically-typed nd-arrays. In addition to the mathematical properties I described above, when \"tensor\" is used to refer to a datatype, it generally means that in addition to being an nd-array, the structure has facilities for tracking what mathematical operators are applied to it to enable [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation). Autodiff is important for ML because the learning procedure is essentially an optimization where you are trying to find the coordinates that minimize a \"cost\" surface that characterizes how good or bad the model is performing on a problem. When the model is performing well, it has a high \"goodness\" score (or rather, a low badness score, aka \"loss\"), which means it's near a low point on the cost surface. To do better, we need to find a position on this surface that is literally downhill from where we are now, and mathematically we get the direction of the slope (the gradient) from differentiation.\n\nThe whole tensor/gradient thing was a bit of a diversion from the embedding stuff, but I thought you'd find that interesting too. If you  found the tensor stuff interesting and want to learn more about what they can do, check out [jax](https://jax.readthedocs.io/en/latest/)", "is_submitter": true, "distinguished": null, "created_utc": 1628778621.0, "parent_id": "t1_h8m1bxo", "replies": [{"author": "WikiSummarizerBot", "id": "h8npxvw", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "", "submission": "p2owt8", "stickied": false, "body": "**[Automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)** \n \n >In mathematics and computer algebra, automatic differentiation (AD), also called algorithmic differentiation, computational differentiation, auto-differentiation, or simply autodiff, is a set of techniques to evaluate the derivative of a function specified by a computer program. AD exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc. ) and elementary functions (exp, log, sin, cos, etc. ).\n \n^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/TheFlyingTree/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)", "is_submitter": false, "distinguished": null, "created_utc": 1628778639.0, "parent_id": "t1_h8npweu", "replies": [{"author": "dogs_like_me", "id": "h8nqszs", "score": 2, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "lol, didn't know there's a bot in here", "is_submitter": true, "distinguished": null, "created_utc": 1628779009.0, "parent_id": "t1_h8npxvw", "replies": []}]}, {"author": "germz80", "id": "h8nzp20", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Seeker #119", "submission": "p2owt8", "stickied": false, "body": "Interesting. This remind me of a classic problem in CS of finding the longest stretch of identical strings between any two given strings, which I think is NP hard. It sounds like this technique might not solve this exact problem in P time, but it essentially does something even more useful in P time, though after you've gathered a ton of data.", "is_submitter": false, "distinguished": null, "created_utc": 1628782743.0, "parent_id": "t1_h8npweu", "replies": []}]}]}]}]}, {"author": "BasilDream", "id": "h8lp7rw", "score": 2, "subreddit": "TheFlyingTree", "author_flair": "Acolyte #163 | Classis Veritatus", "submission": "p2owt8", "stickied": false, "body": "What do you see as the greatest negative to ML/AI?", "is_submitter": false, "distinguished": null, "created_utc": 1628727921.0, "parent_id": "t3_p2owt8", "replies": [{"author": "dogs_like_me", "id": "h8luxce", "score": 2, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "There's an amazing culture of sharing research and code which has resulted in the field developing at a breakneck pace and state-of-the-art tooling being freely available and usable even by high school students. Unfortunately, a lot of the most interesting research areas lend themselves easily to nefarious applications. Consider for example \"object detection.\" The ability to identify objects in images is obviously a super general purpose capability that is broadly applicable. It's an important technique in systems like autonomous vehicles, for detecting things like the edges of the road, other vehicles, pedestrians, road signs, etc. But it also has some pretty self-explanatory military applications, especially things like weapons targetting and... well, autonomous \"vehicles.\" The inventor of one of the most important object detection technologies (I kid you not, it's called \"YOLO\") retired from their research last year because they didn't like the impact they were having on the world. They chimed in on a reddit thread discussing their retirement with some really relevant commentary:\n\n> It's not that the military will use it eventually, it's that the military actively funds research and drives it towards certain goals.\n\n> I'm all for advancing research in the open and this all sounds great in theory. In reality, on a macro scale, research is driven and controlled by people with enormous economic and political power for the general goal of helping them maintain or consolidate that power. Especially in computer science research is driven towards military and industrial uses that have limited benefit to society but enormous benefit to those institutions. Facebook, Google, etc. don't care about driving socially responsible outcomes, they care about profit margins, avoiding regulation, etc.\n\n> I'm not naive, i know that technological advance will continue. Instead of continuing to serve these already powerful institutions I'd rather focus on ways to dismantle the structures of power that drive scientific research towards harmful outcomes for society.\n\nhttps://www.reddit.com/r/MachineLearning/comments/f8wsyg/nd_yolo_creator_joseph_redmon_stopped_cv_research/fioy6nb/?context=3\n\nTL;DR: The biggest negative is that researchers can't help themselves but move forwards technology that is amazing but also easily abused.", "is_submitter": true, "distinguished": null, "created_utc": 1628730611.0, "parent_id": "t1_h8lp7rw", "replies": [{"author": "BasilDream", "id": "h8lvjzu", "score": 3, "subreddit": "TheFlyingTree", "author_flair": "Acolyte #163 | Classis Veritatus", "submission": "p2owt8", "stickied": false, "body": "Wow, thank you for that response.  I guess it really is a double edged sword.", "is_submitter": false, "distinguished": null, "created_utc": 1628730908.0, "parent_id": "t1_h8luxce", "replies": [{"author": "dogs_like_me", "id": "h8lyf5q", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "Buddy, if you think that's bad then you don't know the half of it.\n\nhttps://www.latimes.com/business/technology/story/2021-02-09/dahua-facial-recognition-china-surveillance-uighur", "is_submitter": true, "distinguished": null, "created_utc": 1628732255.0, "parent_id": "t1_h8lvjzu", "replies": [{"author": "BasilDream", "id": "h8n5i2g", "score": 2, "subreddit": "TheFlyingTree", "author_flair": "Acolyte #163 | Classis Veritatus", "submission": "p2owt8", "stickied": false, "body": "Terrifying.", "is_submitter": false, "distinguished": null, "created_utc": 1628768136.0, "parent_id": "t1_h8lyf5q", "replies": []}]}]}]}]}, {"author": "Parking_Pineapple440", "id": "h8lyxsk", "score": 2, "subreddit": "TheFlyingTree", "author_flair": "Seeker #224", "submission": "p2owt8", "stickied": false, "body": "I did random forest regression like once but barely had time to absorb it\u2026 what\u2019s your experience with that?", "is_submitter": false, "distinguished": null, "created_utc": 1628732497.0, "parent_id": "t3_p2owt8", "replies": [{"author": "dogs_like_me", "id": "h8m1kc1", "score": 2, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "It's an amazingly powerful technique that everyone should know about. Honestly, it's probably the closest thing we have to an out-of-the-box, one-size-fits-all algorithm. \n\nIf you want to develop more of an intuition on what it's doing, it's often described as \"an ensemble of weak learners.\" In normal person speak, that means: \"we fit a bunch of really shitty models, but then we combine them all and it works.\" It basically operates off of the same principle as \"The Wisdom Of The Crowd.\" Imagine like walking into a church and hearing the whole crowd singing. It probably sounds pretty good, right? But if you pull any one person aside and have them sing, chances are that random person is off key. But there are probably about as many people who are sharp as are flat, so when they all sing together, it sort of averages out to being right on target.", "is_submitter": true, "distinguished": null, "created_utc": 1628738490.0, "parent_id": "t1_h8lyxsk", "replies": []}]}, {"author": "velcrorex", "id": "h8m3za5", "score": 2, "subreddit": "TheFlyingTree", "author_flair": "Seeker #127", "submission": "p2owt8", "stickied": false, "body": "I know big companies do some interesting things but they also have a lot of computing resources. How much cool stuff can an individual do with just a personal computer?", "is_submitter": false, "distinguished": null, "created_utc": 1628740083.0, "parent_id": "t3_p2owt8", "replies": [{"author": "dogs_like_me", "id": "h8m5xo5", "score": 2, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "Depends a bit on your goal and what kind of computer you have. If you just want to accomplish the thing and it doesn't have to be literally running on your computer, it's pretty easy for a proficient individual to accomplish quite a lot using cloud resources. If you want to do it specifically on your own hardware, if you're a gamer or otherwise have a decent GPU and are patient, you can do most of the coolest stuff. If you don't have a GPU, certain deep learning stuff will take about 4x-10x longer so you'll have to be a lot more patient. Even on a normal laptop you can probably still use pre-trained models others have made available even if it would take you a long time to train yourself. \n\nHere are a few resources to help you see the kinds of crazy stuff that are available to normal people with normal hardware these days:\n\n* https://huggingface.co/transformers/task_summary.html\n* https://paperswithcode.com/\n* https://modelzoo.co/\n* https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n* https://gym.openai.com/\n* https://github.com/iperov/DeepFaceLab", "is_submitter": true, "distinguished": null, "created_utc": 1628741208.0, "parent_id": "t1_h8m3za5", "replies": []}]}, {"author": "vanillaandzombie", "id": "h8n2sax", "score": 2, "subreddit": "TheFlyingTree", "author_flair": "Seeker #154", "submission": "p2owt8", "stickied": false, "body": "High five!\n\nIs geometric deep learning all it\u2019s claimed to be?", "is_submitter": false, "distinguished": null, "created_utc": 1628766287.0, "parent_id": "t3_p2owt8", "replies": [{"author": "dogs_like_me", "id": "h8ng71z", "score": 2, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "Honestly, I'm fairly new to the topic myself but I like the angle they're coming from. I recently started following a free online course offered by some of its biggest proponents if you want to learn more about it: https://geometricdeeplearning.com/\n\nI used to do a lot of graph analytics and if I had a project in front of me right now, a GNN probably wouldn't be the first tool I reached for. I think they probably shine in certain applications, but I'm just not well informed on the topic yet.\n\nI definitely like a lot of the topological and algebraic theory geometric DL researchers are applying to understand what networks are doing.", "is_submitter": true, "distinguished": null, "created_utc": 1628774184.0, "parent_id": "t1_h8n2sax", "replies": []}]}, {"author": "REZJAM_Eric", "id": "h8n7xxw", "score": 2, "subreddit": "TheFlyingTree", "author_flair": "Acolyte #109", "submission": "p2owt8", "stickied": false, "body": "Reading this and your other replies, you know what you're talking about.  \nSincerely,  \nFellow Data Nerd (wouldn't call myself a ML expert, but I know enough to be dangerous)\n\nI'm gonna go philosophical here  \nAgree, disagree, thoughts: Hubert Dreyfus' criticism on AGI feasibility remain valid even with today's neural networks.", "is_submitter": false, "distinguished": null, "created_utc": 1628769667.0, "parent_id": "t3_p2owt8", "replies": [{"author": "dogs_like_me", "id": "h8njje8", "score": 2, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "I actually studied philosophy in undergrad with a focus on philosophy of mind, but I don't remember reading Dreyfus specifically for some reason although I'm sure I must've read at least something of his. Scanning his [WP article](https://geometricdeeplearning.com/), I think a lot of his criticisms about formalization were valid and he'd approve of the modern paradigms of learning representational embeddings and teaching machines to accomplish tasks implicitly rather than trying to show them how we want them to achieve the solution. \n\nI've always been bored by the biological argument, it just feels archaic. Given what we know about biological machinery and computability, it seems necessary that AGI is at least possible without relying on anything special about wetware. \n\nI think the rising interest in causal inference will have consequences for AGI we have yet to foresee, but I still think anything remotely approximating AGI is super far off. I'm honestly not even sure how we AGI is defined right now, considering we've basically smashed the Turing Test (looking at you, GPT-3) but are definitely really far from AGI. \n\nI remember learning about [\"Intentionality\"](https://plato.stanford.edu/entries/intentionality/) back in college and finding the whole thing about propositional attitudes compelling and interesting. Not sure how to address that. \n\nI think a big way that contemporary \"neural\" systems significantly diverge from a system that would be capable of AGI is that they are not embodied. I don't mean to suggest that an AGI would specifically need to be attached to a robot or something like that. But like, an AI is only exhibiting anything resembling intelligence when we choose to interact with it. Invoking the turing test again, I could have a convincing interaction with a GPT-3 chatbot, but when I'm not actually querying the model, it's functionally dead. I could even completely change the behavior and \"opinions\" of the model just by changing the input prompt and not touching the weights at all. I think a necessary component of AGI that maybe hasn't been well formalized yet is some kind of chain reaction or virtuous cycle... the \"thinking\" process needs to have a kind of momentum of its own, continuously responding to both internal and external stimuli. Currently, AI only respond to strictly external stimuli, and without a stimulus to respond to the \"thinking agent\" effectively doesn't exist.", "is_submitter": true, "distinguished": null, "created_utc": 1628775783.0, "parent_id": "t1_h8n7xxw", "replies": [{"author": "REZJAM_Eric", "id": "h8nmdkc", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Acolyte #109", "submission": "p2owt8", "stickied": false, "body": "Well said\n\nI got introduced to Dreyfus' arguments back in 2012 and I'm not a big brain on philosophy so I'm still trying to wrap my head fully around his arguments. He's definitely onto something, but other parts I'm more on the fence about.", "is_submitter": false, "distinguished": null, "created_utc": 1628777076.0, "parent_id": "t1_h8njje8", "replies": [{"author": "dogs_like_me", "id": "h8nvmk5", "score": 2, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "If you're looking for more stuff in that vein, check out the work of Patricia Churchland, Douglas Hofstadter, Daniel Dennett, John Searle, and Thomas Nagel. Less AI and more brain, I also remember being a big fan of V. S. Ramachandran and Oliver Sacks.\n\nOh hey, here's a great collection of essays I bet you'd enjoy: [The Mind's I](https://www.amazon.com/Minds-Fantasies-Reflections-Self-Soul/dp/0465030912)", "is_submitter": true, "distinguished": null, "created_utc": 1628781051.0, "parent_id": "t1_h8nmdkc", "replies": [{"author": "REZJAM_Eric", "id": "h8pily0", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Acolyte #109", "submission": "p2owt8", "stickied": false, "body": "Thanks for the referrals; I'll check them out, especially the Mind's I ones.", "is_submitter": false, "distinguished": null, "created_utc": 1628805237.0, "parent_id": "t1_h8nvmk5", "replies": []}]}]}]}]}, {"author": "An_Dr01d", "id": "h8nlza2", "score": 2, "subreddit": "TheFlyingTree", "author_flair": "Acolyte #66", "submission": "p2owt8", "stickied": false, "body": "What's your ballpark estimate for AGI? Do you think (machine) superintelligence poses an existential threat?", "is_submitter": false, "distinguished": null, "created_utc": 1628776896.0, "parent_id": "t3_p2owt8", "replies": [{"author": "dogs_like_me", "id": "h8ntiwv", "score": 3, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "well outside the lifetime of anyone who reads this message. we're doing lots of cool stuff in ML, but we're really, really far from AGI. I'm not sure if we even have a satisfactory definition of AGI to help drive research towards it or ensure we know we've achieved it when/if we have it.\n\n> Do you think (machine) superintelligence poses an existential threat?\n\nI'll restate this since I clearly don't think AGI poses a threat any time soon.\n\n> Do you think advances in machine learning pose an existential threat?\n\nI had a job once where success in my role was measured by how much I could reduce headcount. If you've ever read articles about how ML automation is coming for your jobs, they meant me. /u/dogs_like_me was coming to automate your job. \n\nAutomation isn't new. It's been accelerating since the industrial revolution. Concerns about unemployment definitely aren't unfounded. Historically, when technology displaces a workforce, new industries and opportunities appear as well. We have way fewer horse farmers today than 200 years ago because cars killed the industry, but it also created a lot of jobs we never could have imagined before cars appeared. Now imagine traveling back in time a few decades and trying to explain what an \"influencer\" is to someone in 1985. \n\nOn the other hand, rate at which we are adopting destabilizing technologies is accelerating too. It was just a few years ago uber appeared and upended the taxi industry, and in just a few more years all of those gig workers might lose their new-found careers to autonomous vehicles. I don't know what the consequences of this will be, but it will probably be a rollercoaster ride and I'm sure the additional economic and political instability that climate change adds to the mix won't help.\n\nBut looking back historically again: humans are weird. Like, really, really weird. We are more technologically advanced than any time in history. The average person's quality of life is probably higher than what royalty experienced hundreds of years ago. We have turned efficiency into a science. And yet, we are somehow working increasingly harder and longer and becoming more stressed and even suicidal too. I think economists call this phenomenon \"induced demand,\" and I don't think it's going away. So although we're experiencing a lot of destabilization, we're good at finding ways to keep ourselves busy and should probably be more worried about working ourselves to death than AI coming for our jobs. \n\nI like to think technology will eventually take us to a star trek-esque economic utopia, but I'm not sure our society will last long enough for that to happen. We'll probably destroy ourselves and revert to something like the dark ages. But if that happens, I don't think it will be technology's fault. I think we'll be to blame.", "is_submitter": true, "distinguished": null, "created_utc": 1628780166.0, "parent_id": "t1_h8nlza2", "replies": []}]}, {"author": "Flesh_Bag", "id": "h8m9omd", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Seeker #95", "submission": "p2owt8", "stickied": false, "body": "What is \"intelligence\"?", "is_submitter": false, "distinguished": null, "created_utc": 1628743427.0, "parent_id": "t3_p2owt8", "replies": [{"author": "dogs_like_me", "id": "h8na1zy", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "In what context?", "is_submitter": true, "distinguished": null, "created_utc": 1628770910.0, "parent_id": "t1_h8m9omd", "replies": [{"author": "Flesh_Bag", "id": "h8rztlj", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Seeker #95", "submission": "p2owt8", "stickied": false, "body": "I mean the \"I\" in AI. But considering \"A\" is for artificial, its complement would be natural intelligence? So I guess I am meaning what is regular intelligence? Then would it be correct to assume that artificial intelligence is just regular intelligence but artificially created by humans?", "is_submitter": false, "distinguished": null, "created_utc": 1628858251.0, "parent_id": "t1_h8na1zy", "replies": []}]}]}, {"author": "risisas", "id": "h8obv3l", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Seeker #280", "submission": "p2owt8", "stickied": false, "body": "this is an obvius one, but to the best of your knowledge, is it possible, given enough time and improvement in technology, to make a bot that gets to pass the re-capcha test?", "is_submitter": false, "distinguished": null, "created_utc": 1628787662.0, "parent_id": "t3_p2owt8", "replies": [{"author": "dogs_like_me", "id": "h8opjhq", "score": 2, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "I think we're already well past that. Captchas are mostly to block low effort hackers. It's like locking the front door to your house: it doesn't do *nothing*, but it probably won't deter anyone who wants to access your house specifically and has some basic knowledge and resources. But it'll for sure keep out some random asshole walking down the street checking houses for unlocked doors.", "is_submitter": true, "distinguished": null, "created_utc": 1628793230.0, "parent_id": "t1_h8obv3l", "replies": [{"author": "risisas", "id": "h8roldd", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Seeker #280", "submission": "p2owt8", "stickied": false, "body": "Yeah, i had imagined that, it was more of a joke question", "is_submitter": false, "distinguished": null, "created_utc": 1628851097.0, "parent_id": "t1_h8opjhq", "replies": []}]}]}, {"author": "haz_mat_", "id": "h8qabwn", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Seeker #193", "submission": "p2owt8", "stickied": false, "body": "Is AI / ML at the point where models have become so complicated that no one really knows how it works?\n\nI remember reading some stuff about these being used in medical diagnosis - apparently it was really good at it too. But I believe they're not moving forward much since a doctor cant verify how a model makes a diagnosis without them also being an expert in convolutional networks and such. Strange times.", "is_submitter": false, "distinguished": null, "created_utc": 1628817984.0, "parent_id": "t3_p2owt8", "replies": [{"author": "olkver", "id": "h8s5lqo", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Seeker #279 | Legio Virtus", "submission": "p2owt8", "stickied": false, "body": "Don't know much about computers.\nDoctors know a lot of things but they can fail in connecting different symptoms and end up with a diagnose.\nPut some information (symptoms) in the computer plus some test results from blood and urin and then the computer will spit out different possibilities of diseases. The doctor might skip a possibility or two to check further up on but the computer will not.\n\nA person in my extended family whent to the doctor with shortness of breath. The doctor said, without looking through the persons history, it must be an anxiety attack or asthma. It turned out that it was congestive heart failure.\n\nLet's say that he went to her medical history on a program on the computer, where different keywords of former medical problems, age, gender etc. and then types \"shortness of breath, I'm pretty shure that with all thr information, congestive heart failure would be on the top of the list to check up on.\n\n[Here](https://www.stlukes-stl.com/health-content/medicine/33/000373.htm) is a list I just found of what shortness of breath could be caused from.\n\nSo to summon up. Doctors know a lot and I have deep respect for the profession. A program like this can be used as a tool for the doctors.", "is_submitter": false, "distinguished": null, "created_utc": 1628861533.0, "parent_id": "t1_h8qabwn", "replies": []}]}, {"author": "helloreddit321567", "id": "h8tv6yt", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Seeker #247", "submission": "p2owt8", "stickied": false, "body": "Hi. Thank you for the post. I would like your opinion on how to nail a mn interview for a Machine Learning/AI position. Also, if you have the time I would have another question about AWS and the use of SSH keys and security groups.", "is_submitter": false, "distinguished": null, "created_utc": 1628887345.0, "parent_id": "t3_p2owt8", "replies": []}, {"author": "SybariteAussie", "id": "h9axxy7", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Acolyte #177", "submission": "p2owt8", "stickied": false, "body": "I didn\u2019t research mbti on YouTube or the internet, had never heard about it. In my YouTube feed a entire row of different podcasts or shows about INFJ\u2019s appeared. How would the YouTube algorithm decide I was this personality type? Solely on the content I watch & listen to. The selection of music. Accessing my email & messages, accessing what I write in the notes section on my iPad or all of the above? I took a free mbti assessment that took about 15 mins. Said infj. I didn\u2019t want that result so took a different test a few days later & got infp that took 10 mins. Paid for official assessment that took over an hour. Said infj. How would the YouTube algorithm arrive at this conclusion?", "is_submitter": false, "distinguished": null, "created_utc": 1629215709.0, "parent_id": "t3_p2owt8", "replies": [{"author": "dogs_like_me", "id": "h9bcdoy", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "They didn't. They only inferred that you might find that content interesting. Presumably, other people who watch shows about INFJs engage with similar content as you to. Simple as that.\n\nSomething to consider: there's probably a big distinction between people for whom INFJ is an appropriate way to label their personality types, vs. people who actively engage with youtube content about the INFJ experience. Granted, I'm sure most of the latter group fall into the former as well. But I think it's safe to assume that the vast majority of people who might be applicable to some particular personality type label aren't themselves interested in personality type theory generally. Having a psychological attribute doesn't make someone interested in psychology. We all have personalities, and yet most people aren't interested in psychology at all.\n\nI hope this distinction helps clarify the difference between youtube labeling you as INFJ vs. youtube labeling you as someone who might be interested in INFJ videos.", "is_submitter": true, "distinguished": null, "created_utc": 1629223212.0, "parent_id": "t1_h9axxy7", "replies": [{"author": "SybariteAussie", "id": "h9cs8c7", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Acolyte #177", "submission": "p2owt8", "stickied": false, "body": "I am interested in psychology and follow several psychologists on YouTube. Me being interested in mbti is plausible. Why only have one mbti type in the feed? There are 15 other mbti types that would also interest me. Yet the row contained only content on infj. Initially I scoffed at the concept that I was a introvert. I worked in fine dining restaurants for 20 years as waiter, barman or manager. My job was talking to people all day & night. I anticipated different results from taking mbti assessment. In hindsight my behaviour & arguments against the assessment strangely seem to confirm it. I took another free test then paid for a assessment. Apparently infj males are rare. It would explain my contradictory features, my overthinking & some other negative features. I was also watching The hated one & Rob Braxman on YouTube. Discovered that everything I do on iPad is recorded & stored. Even if I delete it. I used to start each day setting daily goals, berating or encouraging myself in the notes section. Writing my thoughts or asking questions. Vent negative thoughts & emotions. I would then respond to this from different perspectives. Sometimes these debates with myself would last weeks & contain many pages. I would entice myself with conditional rewards & threaten myself with punishments. Write lengthy poetry. Then delete everything. I now use notepads to do these things. Burn the pages in the laundry sink afterwards. If YouTube was accessing my activity in the notes section,my emails,my text messages & internet search history it would explain the algorithm reaching this conclusion.", "is_submitter": false, "distinguished": null, "created_utc": 1629246747.0, "parent_id": "t1_h9bcdoy", "replies": [{"author": "dogs_like_me", "id": "h9dpvst", "score": 1, "subreddit": "TheFlyingTree", "author_flair": "Seeker #156", "submission": "p2owt8", "stickied": false, "body": "> If YouTube was accessing my activity in the notes section,my emails,my text messages & internet search history it would explain the algorithm reaching this conclusion.\n\npretty confident that's not what's going on here. If they're getting any non-youtube data of your activity, my assumption would be a linked google account. \n\nI wouldn't worry about it or overthink it.", "is_submitter": true, "distinguished": null, "created_utc": 1629266526.0, "parent_id": "t1_h9cs8c7", "replies": []}]}]}]}]}